{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "756c2d8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import GCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e4d2c42d",
   "metadata": {},
   "outputs": [],
   "source": [
    "adjacencyList = np.load(\"filtered_data/hseAdjacencyList.npy\", allow_pickle=True)\n",
    "inputFeatureVectorList = np.load(\"filtered_data/inputFeatureVector.npy\")\n",
    "outputLabelList = np.load(\"filtered_data/outputFeatureVector.npy\", allow_pickle=True)\n",
    "\n",
    "# adjacencyList = [np.array(adjacencyList[i], dtype=np.double).T for i in range(len(adjacencyList))]\n",
    "\n",
    "adjMatrix = np.load(\"filtered_data/hseAdjacencyMatrix.npy\", allow_pickle=True)\n",
    "\n",
    "inputFeatureVectorList = inputFeatureVectorList.astype(np.double)\n",
    "\n",
    "adjMatrix = np.where(adjMatrix < .25, 1, 0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "83a29422",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reload the gcn lib\n",
    "import GCN\n",
    "# load the data\n",
    "# load the data\n",
    "from torch_geometric.utils import dense_to_sparse\n",
    "\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.data import Data\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def trainModel(outputLabel, device, epochs, verbose=True, save=False):\n",
    "\n",
    "    # should likely be done all as a preprocessing step however it takes so little of the time it doesnt matter that much\n",
    "\n",
    "    labels = np.array([outputLabelList[i][outputLabel] for i in range(len(outputLabelList))], dtype=np.double)\n",
    "\n",
    "    # normalize the labels\n",
    "    labels = (labels - np.min(labels)) / (np.max(labels) - np.min(labels))\n",
    "\n",
    "\n",
    "    # Split data into train and validation sets (70% train, 30% val)\n",
    "    X_train, X_val, adj_train, adj_val, y_train, y_val = train_test_split(\n",
    "        inputFeatureVectorList, adjMatrix, labels, test_size=0.3, random_state=42\n",
    "    )\n",
    "\n",
    "    # Convert everything to a tensor\n",
    "    inputFeatureTensor_train = torch.tensor(X_train, dtype=torch.float).to(device)\n",
    "    inputFeatureTensor_val = torch.tensor(X_val, dtype=torch.float).to(device)\n",
    "    adjMatrixTensor_train = torch.tensor(adj_train, dtype=torch.float).to(device)\n",
    "    adjMatrixTensor_val = torch.tensor(adj_val, dtype=torch.float).to(device)\n",
    "    labelsTensor_train = torch.tensor(y_train, dtype=torch.float).to(device)\n",
    "    labelsTensor_val = torch.tensor(y_val, dtype=torch.float).to(device)\n",
    "\n",
    "    # Prepare datasets and loaders\n",
    "    data_set_train = GCN.InterpreterDataset(\n",
    "        inputFeatureTensor_train,\n",
    "        adjMatrixTensor_train,\n",
    "        labelsTensor_train,\n",
    "    )\n",
    "    data_set_val = GCN.InterpreterDataset(\n",
    "        inputFeatureTensor_val,\n",
    "        adjMatrixTensor_val,\n",
    "        labelsTensor_val,\n",
    "    )\n",
    "\n",
    "\n",
    "    data_loader_train = DataLoader(data_set_train, batch_size=100, shuffle=True)\n",
    "    data_loader_val = DataLoader(data_set_val, batch_size=100, shuffle=False)\n",
    "\n",
    "    #print length of the dataset\n",
    "    print(f\"Training on {len(data_loader_train.dataset)} samples\")\n",
    "    print(f\"Validating on {len(data_loader_val.dataset)} samples\")\n",
    "\n",
    "    model = GCN.GraphConv(len(inputFeatureVectorList[0][0]), .001).to(device)\n",
    "    model.trainModel(data_loader_train, data_loader_val, epochs, verbose=verbose)\n",
    "\n",
    "    if save:\n",
    "        torch.save(model.state_dict(), f\"./models/model_{outputLabel}.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "21915f18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n",
      "Training model for defect_fermi_energy\n",
      "Training on 5406 samples\n",
      "Validating on 2317 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/30:   0%|          | 0/55 [00:00<?, ?it/s]/home/petedowney/miniconda3/envs/MLQuanta/lib/python3.10/site-packages/torch/nn/modules/loss.py:538: UserWarning: Using a target size (torch.Size([100])) that is different to the input size (torch.Size([10000, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "Epoch 1/30:  98%|█████████▊| 54/55 [00:03<00:00, 16.49it/s, loss=0.0416]/home/petedowney/miniconda3/envs/MLQuanta/lib/python3.10/site-packages/torch/nn/modules/loss.py:538: UserWarning: Using a target size (torch.Size([6])) that is different to the input size (torch.Size([600, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "Epoch 1/30: 100%|██████████| 55/55 [00:03<00:00, 15.52it/s, loss=0.042] \n",
      "/home/petedowney/miniconda3/envs/MLQuanta/lib/python3.10/site-packages/torch/nn/modules/loss.py:538: UserWarning: Using a target size (torch.Size([17])) that is different to the input size (torch.Size([1700, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "Epoch 2/30: 100%|██████████| 55/55 [00:02<00:00, 21.27it/s, loss=0.0439]\n",
      "Epoch 3/30: 100%|██████████| 55/55 [00:02<00:00, 18.61it/s, loss=0.0437]\n",
      "Epoch 4/30: 100%|██████████| 55/55 [00:03<00:00, 17.37it/s, loss=0.0378]\n",
      "Epoch 5/30: 100%|██████████| 55/55 [00:02<00:00, 21.78it/s, loss=0.0457]\n",
      "Epoch 6/30: 100%|██████████| 55/55 [00:02<00:00, 22.31it/s, loss=0.0398]\n",
      "Epoch 7/30: 100%|██████████| 55/55 [00:02<00:00, 22.30it/s, loss=0.0446]\n",
      "Epoch 8/30: 100%|██████████| 55/55 [00:02<00:00, 23.88it/s, loss=0.042] \n",
      "Epoch 9/30: 100%|██████████| 55/55 [00:03<00:00, 14.37it/s, loss=0.0413]\n",
      "Epoch 10/30: 100%|██████████| 55/55 [00:04<00:00, 13.45it/s, loss=0.0461]\n",
      "Epoch 11/30: 100%|██████████| 55/55 [00:03<00:00, 14.10it/s, loss=0.0431]\n",
      "Epoch 12/30: 100%|██████████| 55/55 [00:03<00:00, 16.30it/s, loss=0.0423]\n",
      "Epoch 13/30: 100%|██████████| 55/55 [00:02<00:00, 24.11it/s, loss=0.0493]\n",
      "Epoch 14/30: 100%|██████████| 55/55 [00:02<00:00, 24.43it/s, loss=0.0423]\n",
      "Epoch 15/30: 100%|██████████| 55/55 [00:02<00:00, 24.31it/s, loss=0.0399]\n",
      "Epoch 16/30: 100%|██████████| 55/55 [00:02<00:00, 23.88it/s, loss=0.045] \n",
      "Epoch 17/30: 100%|██████████| 55/55 [00:02<00:00, 22.56it/s, loss=0.0439]\n",
      "Epoch 18/30: 100%|██████████| 55/55 [00:02<00:00, 24.28it/s, loss=0.0426]\n",
      "Epoch 19/30: 100%|██████████| 55/55 [00:02<00:00, 23.65it/s, loss=0.0465]\n",
      "Epoch 20/30: 100%|██████████| 55/55 [00:02<00:00, 24.85it/s, loss=0.0431]\n",
      "Epoch 21/30: 100%|██████████| 55/55 [00:02<00:00, 24.16it/s, loss=0.0458]\n",
      "Epoch 22/30: 100%|██████████| 55/55 [00:02<00:00, 23.34it/s, loss=0.0407]\n",
      "Epoch 23/30: 100%|██████████| 55/55 [00:02<00:00, 24.75it/s, loss=0.0436]\n",
      "Epoch 24/30: 100%|██████████| 55/55 [00:02<00:00, 24.77it/s, loss=0.0165]\n",
      "Epoch 25/30: 100%|██████████| 55/55 [00:02<00:00, 24.16it/s, loss=0.00156] \n",
      "Epoch 26/30: 100%|██████████| 55/55 [00:02<00:00, 24.61it/s, loss=0.00027] \n",
      "Epoch 27/30: 100%|██████████| 55/55 [00:02<00:00, 24.23it/s, loss=0.000153]\n",
      "Epoch 28/30: 100%|██████████| 55/55 [00:02<00:00, 22.93it/s, loss=0.00179] \n",
      "Epoch 29/30: 100%|██████████| 55/55 [00:02<00:00, 22.87it/s, loss=0.00022] \n",
      "Epoch 30/30: 100%|██████████| 55/55 [00:02<00:00, 23.84it/s, loss=0.000153]\n"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "importlib.reload(GCN)\n",
    "\n",
    "\n",
    "trainLabelList = [\"defect_fermi_energy\"]\n",
    "\n",
    "# trainLabelList =[\n",
    "#     \"defect_total_energy\",\"defect_energy_convergence\", \"defect_fermi_energy\", \"defect_vbm_energy\", \n",
    "#     \"defect_cbm_energy\", \"defect_cbm-vbm_energy\", \"defect_spin_state\", \"defect_max_hf\", \n",
    "#     \"defect_max_hf_asymmetry\", \"defect_dos_at_fermi_energy\", \"defect_dos_at_vbm_energy\",\n",
    "#     \"defect_dos_at_cbm_energy\",\"defect_bulk_gap\", \"defect_lvl_delta\", \"defect_lvl_vbm_delta\",\n",
    "#     \"defect_lvl_cbm_delta\", \"defect_num_occ_lvl\", \"defect_num_unocc_lvl\",\"defect_occ_min_localization\",\n",
    "#     \"defect_unocc_min_localization\", \"defect_occ_avg_localization\", \"defect_unocc_avg_localization\",\n",
    "#     \"defect_lvl_tdm\"]\n",
    "\n",
    "\n",
    "# Use CUDA if available\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "\n",
    "for label in trainLabelList:\n",
    "    print(f\"Training model for {label}\")\n",
    "    trainModel(label, device, 30, save=True, verbose=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MLQuanta",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
